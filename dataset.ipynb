{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2272314f",
   "metadata": {},
   "source": [
    "Peak Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a46fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio normalization process...\n",
      "Input Directory:  Datasets/\n",
      "Output Directory: DatasetNormalized/\n",
      "Found 864 .wav files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing audio files: 100%|██████████| 864/864 [00:02<00:00, 375.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalization process complete!\n",
      "Normalized files are saved in: DatasetNormalized/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_audio_file(input_path, output_path, target_peak=0.98):\n",
    "\n",
    "    try:\n",
    "        # Load the audio file, preserving its original sample rate\n",
    "        audio, sr = librosa.load(input_path, sr=None)\n",
    "\n",
    "        # Skip silent files\n",
    "        if np.max(np.abs(audio)) == 0:\n",
    "            # Just copy the silent file to the destination\n",
    "            sf.write(output_path, audio, sr)\n",
    "            return \"Silent\"\n",
    "\n",
    "        # Perform peak normalization\n",
    "        peak = np.max(np.abs(audio))\n",
    "        scaling_factor = target_peak / peak\n",
    "        normalized_audio = audio * scaling_factor\n",
    "        \n",
    "        # Save the normalized audio\n",
    "        sf.write(output_path, normalized_audio, sr)\n",
    "        \n",
    "        return \"Success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def process_directory(input_dir, output_dir, target_peak=0.98):\n",
    "   \n",
    "    # --- 1. Find all audio files to process ---\n",
    "    audio_files = []\n",
    "    # os.walk will go through the input_dir and all its subdirectories\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            # Process only .wav files (adjust if you have other formats)\n",
    "            if filename.lower().endswith('.wav'):\n",
    "                audio_files.append(os.path.join(root, filename))\n",
    "\n",
    "    if not audio_files:\n",
    "        print(\"No .wav files found in the specified input directory. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(audio_files)} .wav files to process.\")\n",
    "\n",
    "    # --- 2. Process files with a progress bar ---\n",
    "    # tqdm creates the visual progress bar\n",
    "    for input_path in tqdm(audio_files, desc=\"Normalizing audio files\"):\n",
    "        # Create the corresponding output path, preserving sub-folder structure\n",
    "        relative_path = os.path.relpath(input_path, input_dir)\n",
    "        output_path = os.path.join(output_dir, relative_path)\n",
    "        \n",
    "        # Ensure the output sub-directory exists\n",
    "        output_sub_dir = os.path.dirname(output_path)\n",
    "        os.makedirs(output_sub_dir, exist_ok=True)\n",
    "        \n",
    "        # Normalize the individual file\n",
    "        status = normalize_audio_file(input_path, output_path, target_peak)\n",
    "        \n",
    "        if status != \"Success\" and status != \"Silent\":\n",
    "            # Optional: Print errors for files that failed\n",
    "            print(f\"Could not process {input_path}. Reason: {status}\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # ---  USER CONFIGURATION ---\n",
    "    # 1. Set the path to your original SVD dataset\n",
    "    INPUT_FOLDER = \"Datasets/\"\n",
    "    \n",
    "    # 2. Set the path where the new normalized dataset will be saved\n",
    "    OUTPUT_FOLDER = \"DatasetNormalized/\"\n",
    "    # --- END CONFIGURATION ---\n",
    "\n",
    "    print(\"Starting audio normalization process...\")\n",
    "    print(f\"Input Directory:  {INPUT_FOLDER}\")\n",
    "    print(f\"Output Directory: {OUTPUT_FOLDER}\")\n",
    "    \n",
    "    process_directory(INPUT_FOLDER, OUTPUT_FOLDER)\n",
    "    \n",
    "    print(\"\\nNormalization process complete!\")\n",
    "    print(f\"Normalized files are saved in: {OUTPUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b2f797",
   "metadata": {},
   "source": [
    "Silence Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9f8691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Optional: Suppress librosa warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='librosa')\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d014577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Directory (normalized files): 'DatasetNormalized/'\n",
      "Output Directory (trimmed files):   'DatasetTrimmed/'\n",
      "Silence Threshold: 30 dB\n"
     ]
    }
   ],
   "source": [
    "# ---  USER CONFIGURATION ---\n",
    "\n",
    "# 1. Set the path to your NORMALIZED dataset (the output from the previous step)\n",
    "INPUT_FOLDER = \"DatasetNormalized/\"\n",
    "\n",
    "# 2. Set the path where the new trimmed (silence-removed) dataset will be saved\n",
    "OUTPUT_FOLDER = \"DatasetTrimmed/\"\n",
    "\n",
    "# 3. Set the silence threshold in decibels (dB) below the peak amplitude.\n",
    "#    A higher value (e.g., 40) is less aggressive.\n",
    "#    A lower value (e.g., 20) is more aggressive.\n",
    "#    30 is a good starting point.\n",
    "SILENCE_THRESHOLD_DB = 30\n",
    "\n",
    "# --- END CONFIGURATION ---\n",
    "\n",
    "# Create the output directory to make sure it exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f\"Input Directory (normalized files): '{INPUT_FOLDER}'\")\n",
    "print(f\"Output Directory (trimmed files):   '{OUTPUT_FOLDER}'\")\n",
    "print(f\"Silence Threshold: {SILENCE_THRESHOLD_DB} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57623de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silence removal function defined.\n"
     ]
    }
   ],
   "source": [
    "def trim_silence_from_file(input_path, output_path, top_db):\n",
    "   \n",
    "    try:\n",
    "        # Load the audio file\n",
    "        audio, sr = librosa.load(input_path, sr=None)\n",
    "\n",
    "        # librosa.effects.split returns the start and end indices of non-silent chunks\n",
    "        non_silent_intervals = librosa.effects.split(audio, top_db=top_db)\n",
    "\n",
    "        # Concatenate the non-silent audio chunks together\n",
    "        trimmed_audio = np.concatenate([audio[start:end] for start, end in non_silent_intervals])\n",
    "        \n",
    "        # Handle cases where the file becomes empty after trimming\n",
    "        if len(trimmed_audio) == 0:\n",
    "            # Save a tiny amount of silence to avoid creating an empty file\n",
    "            trimmed_audio = np.zeros(1, dtype=np.float32)\n",
    "            sf.write(output_path, trimmed_audio, sr)\n",
    "            return \"Silent_File\"\n",
    "        \n",
    "        # Save the trimmed audio\n",
    "        sf.write(output_path, trimmed_audio, sr)\n",
    "        return \"Success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "print(\"Silence removal function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55acf71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 .wav files. Starting silence removal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trimming silence: 100%|██████████| 864/864 [00:09<00:00, 88.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "Silence removal process complete!\n",
      "All trimmed files have been saved in: 'DatasetTrimmed/'\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find all audio files to process ---\n",
    "audio_files_to_process = []\n",
    "for root, _, files in os.walk(INPUT_FOLDER):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith('.wav'):\n",
    "            full_path = os.path.join(root, filename)\n",
    "            audio_files_to_process.append(full_path)\n",
    "\n",
    "if not audio_files_to_process:\n",
    "    print(f\"Warning: No .wav files were found in '{INPUT_FOLDER}'. Please check the path.\")\n",
    "else:\n",
    "    print(f\"Found {len(audio_files_to_process)} .wav files. Starting silence removal...\")\n",
    "\n",
    "    # --- 2. Process files with a progress bar ---\n",
    "    for input_path in tqdm(audio_files_to_process, desc=\"Trimming silence\"):\n",
    "        # Create the corresponding output path, preserving the sub-folder structure\n",
    "        relative_path = os.path.relpath(input_path, INPUT_FOLDER)\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, relative_path)\n",
    "        \n",
    "        # Ensure the output sub-directory exists before saving\n",
    "        output_sub_dir = os.path.dirname(output_path)\n",
    "        os.makedirs(output_sub_dir, exist_ok=True)\n",
    "        \n",
    "        # Trim silence from the individual file\n",
    "        status = trim_silence_from_file(input_path, output_path, SILENCE_THRESHOLD_DB)\n",
    "        \n",
    "        if status.startswith(\"Error\"):\n",
    "            print(f\"Could not process {input_path}. Reason: {status}\")\n",
    "\n",
    "    print(\"\\n------------------------------------\")\n",
    "    print(\"Silence removal process complete!\")\n",
    "    print(f\"All trimmed files have been saved in: '{OUTPUT_FOLDER}'\")\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84d49c",
   "metadata": {},
   "source": [
    "Merge small Classes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75473dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Analyzing Dataset ---\n",
      "\n",
      "[INFO] File counts per class:\n",
      "  - Dysarthia: 130 files\n",
      "  - Dysphonie: 130 files\n",
      "  - Laryngitis: 130 files\n",
      "  - parkinson: 130 files\n",
      "  - spasmodische_dysphonie: 130 files\n",
      "  - Vox senilis: 130 files\n",
      "  - Laryngozele: 84 files\n",
      "\n",
      "[INFO] Reorganization Plan:\n",
      "Threshold set to: 50 files.\n",
      "No classes fall below the threshold. All classes will be copied as-is.\n",
      "The following 7 classes will be KEPT SEPARATE:\n",
      "  ['Dysarthia', 'Dysphonie', 'Laryngitis', 'Laryngozele', 'parkinson', 'spasmodische_dysphonie', 'Vox senilis']\n",
      "\n",
      "--- Phase 2: Executing Reorganization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reorganizing classes: 100%|██████████| 7/7 [00:00<00:00, 32.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "      Automated reorganization complete!      \n",
      "The final dataset is ready for use in:\n",
      "'Dataset_AutoMerged'\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm  # <-- THIS IS THE ONLY LINE THAT HAS BEEN CHANGED\n",
    "import warnings\n",
    "\n",
    "# Optional: Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#                           USER CONFIGURATION\n",
    "# --------------------------------------------------------------------------\n",
    "# ---  Please edit the variables below ---\n",
    "\n",
    "# 1. Set the path to your trimmed dataset folder.\n",
    "SOURCE_DATASET_FOLDER = \"DatasetTrimmed\"\n",
    "\n",
    "# 2. Set the path where the new, auto-merged dataset will be saved.\n",
    "DESTINATION_DATASET_FOLDER = \"Dataset_AutoMerged\"\n",
    "\n",
    "# 3. !! IMPORTANT: Set your merging threshold.\n",
    "#    Any class with FEWER files than this number will be automatically merged.\n",
    "MERGE_THRESHOLD = 50 \n",
    "\n",
    "# 4. Give a name to the new, combined class folder for the small classes.\n",
    "NEW_CLASS_NAME = \"Other_Pathologies\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#                         MAIN PROCESSING SCRIPT\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- Phase 1: Analyze the Dataset ---\n",
    "print(\"--- Phase 1: Analyzing Dataset ---\")\n",
    "\n",
    "class_counts = {}\n",
    "try:\n",
    "    source_classes = [d for d in os.listdir(SOURCE_DATASET_FOLDER) if os.path.isdir(os.path.join(SOURCE_DATASET_FOLDER, d))]\n",
    "    if not source_classes:\n",
    "        raise FileNotFoundError(f\"No subdirectories found in '{SOURCE_DATASET_FOLDER}'.\")\n",
    "\n",
    "    for class_name in source_classes:\n",
    "        class_path = os.path.join(SOURCE_DATASET_FOLDER, class_name)\n",
    "        num_files = len([f for f in os.listdir(class_path) if f.lower().endswith('.wav')])\n",
    "        class_counts[class_name] = num_files\n",
    "\n",
    "    print(\"\\n[INFO] File counts per class:\")\n",
    "    sorted_counts = sorted(class_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    for name, count in sorted_counts:\n",
    "        print(f\"  - {name}: {count} files\")\n",
    "\n",
    "    classes_to_merge = [name for name, count in class_counts.items() if count < MERGE_THRESHOLD]\n",
    "    classes_to_keep = [name for name, count in class_counts.items() if count >= MERGE_THRESHOLD]\n",
    "\n",
    "    print(\"\\n[INFO] Reorganization Plan:\")\n",
    "    print(f\"Threshold set to: {MERGE_THRESHOLD} files.\")\n",
    "    if classes_to_merge:\n",
    "        print(f\"The following {len(classes_to_merge)} classes will be MERGED into '{NEW_CLASS_NAME}':\")\n",
    "        print(f\"  {classes_to_merge}\")\n",
    "    else:\n",
    "        print(\"No classes fall below the threshold. All classes will be copied as-is.\")\n",
    "    \n",
    "    if classes_to_keep:\n",
    "         print(f\"The following {len(classes_to_keep)} classes will be KEPT SEPARATE:\")\n",
    "         print(f\"  {classes_to_keep}\")\n",
    "\n",
    "    # --- Phase 2: Execute the Reorganization ---\n",
    "    print(\"\\n--- Phase 2: Executing Reorganization ---\")\n",
    "    os.makedirs(DESTINATION_DATASET_FOLDER, exist_ok=True)\n",
    "    \n",
    "    for class_name in tqdm(source_classes, desc=\"Reorganizing classes\"):\n",
    "        source_class_path = os.path.join(SOURCE_DATASET_FOLDER, class_name)\n",
    "        \n",
    "        if class_name in classes_to_merge:\n",
    "            destination_class_path = os.path.join(DESTINATION_DATASET_FOLDER, NEW_CLASS_NAME)\n",
    "        else:\n",
    "            destination_class_path = os.path.join(DESTINATION_DATASET_FOLDER, class_name)\n",
    "            \n",
    "        os.makedirs(destination_class_path, exist_ok=True)\n",
    "        \n",
    "        files_to_copy = [f for f in os.listdir(source_class_path) if f.lower().endswith('.wav')]\n",
    "        for filename in files_to_copy:\n",
    "            source_file_path = os.path.join(source_class_path, filename)\n",
    "            destination_file_path = os.path.join(destination_class_path, filename)\n",
    "            shutil.copy2(source_file_path, destination_file_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"      Automated reorganization complete!      \")\n",
    "    print(f\"The final dataset is ready for use in:\\n'{DESTINATION_DATASET_FOLDER}'\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "    print(\"Please check your folder paths and ensure the source directory is structured correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16862cc9",
   "metadata": {},
   "source": [
    "Split Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802fa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Validating Configuration and Collecting Files ---\n",
      "Found 7 classes: ['Dysarthia', 'Dysphonie', 'Laryngitis', 'Laryngozele', 'parkinson', 'spasmodische_dysphonie', 'Vox senilis']\n",
      "Collected a total of 864 files.\n",
      "\n",
      "--- Phase 2: Performing Stratified Split ---\n",
      "Split successful. Planned distribution:\n",
      "  - Training samples:   691\n",
      "  - Validation samples: 86\n",
      "  - Test samples:       87\n",
      "\n",
      "--- Phase 3: Copying Files to Destination ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to train: 100%|██████████| 691/691 [00:00<00:00, 2845.41it/s]\n",
      "Copying to validation: 100%|██████████| 86/86 [00:00<00:00, 3232.89it/s]\n",
      "Copying to test: 100%|██████████| 87/87 [00:00<00:00, 3412.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 4: Verifying Final File Counts ---\n",
      "Final distribution of files:\n",
      "\n",
      "Train Set:\n",
      "  - Dysarthia: 403 files\n",
      "  - Dysphonie: 410 files\n",
      "  - Laryngitis: 409 files\n",
      "  - Laryngozele: 260 files\n",
      "  - Vox senilis: 403 files\n",
      "  - parkinson: 409 files\n",
      "  - spasmodische_dysphonie: 412 files\n",
      "  ------------------\n",
      "  Total: 2706 files\n",
      "\n",
      "Validation Set:\n",
      "  - Dysarthia: 13 files\n",
      "  - Dysphonie: 13 files\n",
      "  - Laryngitis: 13 files\n",
      "  - Laryngozele: 8 files\n",
      "  - Vox senilis: 13 files\n",
      "  - parkinson: 13 files\n",
      "  - spasmodische_dysphonie: 13 files\n",
      "  ------------------\n",
      "  Total: 86 files\n",
      "\n",
      "Test Set:\n",
      "  - Dysarthia: 13 files\n",
      "  - Dysphonie: 13 files\n",
      "  - Laryngitis: 13 files\n",
      "  - Laryngozele: 9 files\n",
      "  - Vox senilis: 13 files\n",
      "  - parkinson: 13 files\n",
      "  - spasmodische_dysphonie: 13 files\n",
      "  ------------------\n",
      "  Total: 87 files\n",
      "\n",
      "==================================================\n",
      "      Dataset splitting process complete!      \n",
      "The final dataset is ready for training in:\n",
      "'Dataset_For_Training'\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Optional: Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#                           USER CONFIGURATION\n",
    "# --------------------------------------------------------------------------\n",
    "# ---  Please edit the variables below ---\n",
    "\n",
    "# 1. Set the path to your final, merged, and preprocessed dataset.\n",
    "#    This should be the output from the previous merging step.\n",
    "SOURCE_FOLDER = \"Dataset_AutoMerged\"\n",
    "\n",
    "# 2. Set the path where the final split dataset will be created.\n",
    "DESTINATION_FOLDER = \"Dataset_For_Training\"\n",
    "\n",
    "# 3. Define the split ratios. These MUST sum to 1.0.\n",
    "TRAIN_SIZE = 0.8  # 80%\n",
    "VAL_SIZE = 0.1    # 10%\n",
    "TEST_SIZE = 0.1   # 10%\n",
    "\n",
    "# 4. Set a random state for reproducibility of the split.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#                         MAIN PROCESSING SCRIPT\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- 1. Validation and Setup ---\n",
    "print(\"--- Phase 1: Validating Configuration and Collecting Files ---\")\n",
    "\n",
    "if not np.isclose(TRAIN_SIZE + VAL_SIZE + TEST_SIZE, 1.0):\n",
    "    print(f\"Error: Split ratios must sum to 1.0. Current sum is {TRAIN_SIZE + VAL_SIZE + TEST_SIZE}\")\n",
    "else:\n",
    "    # Create the main destination directory\n",
    "    os.makedirs(DESTINATION_FOLDER, exist_ok=True)\n",
    "    \n",
    "    # --- 2. Collect all file paths and their corresponding labels ---\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    \n",
    "    classes = [d for d in os.listdir(SOURCE_FOLDER) if os.path.isdir(os.path.join(SOURCE_FOLDER, d))]\n",
    "    print(f\"Found {len(classes)} classes: {classes}\")\n",
    "\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(SOURCE_FOLDER, cls)\n",
    "        files = os.listdir(class_path)\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.wav'):\n",
    "                filepaths.append(os.path.join(class_path, file))\n",
    "                labels.append(cls)\n",
    "\n",
    "    print(f\"Collected a total of {len(filepaths)} files.\")\n",
    "\n",
    "    # --- 3. Perform the stratified split ---\n",
    "    print(\"\\n--- Phase 2: Performing Stratified Split ---\")\n",
    "    \n",
    "    # First split: separate out the training set\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        filepaths, labels,\n",
    "        train_size=TRAIN_SIZE,\n",
    "        stratify=labels,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Second split: split the remainder into validation and test sets\n",
    "    # Calculate the ratio for the second split\n",
    "    relative_test_size = TEST_SIZE / (VAL_SIZE + TEST_SIZE)\n",
    "    \n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=relative_test_size,\n",
    "        stratify=y_temp,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    print(\"Split successful. Planned distribution:\")\n",
    "    print(f\"  - Training samples:   {len(X_train)}\")\n",
    "    print(f\"  - Validation samples: {len(X_val)}\")\n",
    "    print(f\"  - Test samples:       {len(X_test)}\")\n",
    "\n",
    "    # --- 4. Copy files to the new directory structure ---\n",
    "    print(\"\\n--- Phase 3: Copying Files to Destination ---\")\n",
    "\n",
    "    def copy_files(file_list, destination_name):\n",
    "        destination_path = os.path.join(DESTINATION_FOLDER, destination_name)\n",
    "        os.makedirs(destination_path, exist_ok=True)\n",
    "        \n",
    "        for file_path in tqdm(file_list, desc=f\"Copying to {destination_name}\"):\n",
    "            class_name = os.path.basename(os.path.dirname(file_path))\n",
    "            destination_class_path = os.path.join(destination_path, class_name)\n",
    "            os.makedirs(destination_class_path, exist_ok=True)\n",
    "            shutil.copy2(file_path, destination_class_path)\n",
    "\n",
    "    copy_files(X_train, \"train\")\n",
    "    copy_files(X_val, \"validation\")\n",
    "    copy_files(X_test, \"test\")\n",
    "\n",
    "    # --- 5. Final Verification ---\n",
    "    print(\"\\n--- Phase 4: Verifying Final File Counts ---\")\n",
    "    final_counts = {}\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        split_path = os.path.join(DESTINATION_FOLDER, split)\n",
    "        final_counts[split] = {}\n",
    "        for cls in os.listdir(split_path):\n",
    "            count = len(os.listdir(os.path.join(split_path, cls)))\n",
    "            final_counts[split][cls] = count\n",
    "    \n",
    "    print(\"Final distribution of files:\")\n",
    "    for split, class_data in final_counts.items():\n",
    "        print(f\"\\n{split.capitalize()} Set:\")\n",
    "        total = 0\n",
    "        for cls, count in sorted(class_data.items()):\n",
    "            print(f\"  - {cls}: {count} files\")\n",
    "            total += count\n",
    "        print(f\"  ------------------\\n  Total: {total} files\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"      Dataset splitting process complete!      \")\n",
    "    print(f\"The final dataset is ready for training in:\\n'{DESTINATION_FOLDER}'\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f1764",
   "metadata": {},
   "source": [
    "Augment Train Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ad2c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Augmentation Script Initializing ---\n",
      "Info: RIR folder not found or not specified. Reverb augmentation will be disabled.\n",
      "\n",
      "--- Phase 1: Collecting Original Training Files ---\n",
      "Found 691 original audio files to augment.\n",
      "\n",
      "--- Phase 2: Applying Augmentation Pipelines ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting files: 100%|██████████| 691/691 [00:19<00:00, 35.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "       Data augmentation complete! ✨\n",
      "Successfully created 2073 new training files.\n",
      "Your training folders now contain a diverse mix of augmented data.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "from scipy.signal import butter, lfilter, convolve\n",
    "\n",
    "# Optional: Suppress warnings from librosa, etc.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#                           USER CONFIGURATION\n",
    "# --------------------------------------------------------------------------\n",
    "# --- Please edit the variables below ---\n",
    "\n",
    "# 1. Set the path to the 'train' subfolder of your dataset.\n",
    "TRAIN_DATA_FOLDER = \"Dataset_For_Training/train\"\n",
    "\n",
    "# 2. Set the path to a folder containing Room Impulse Response (RIR) WAV files for reverb.\n",
    "#    You can download free, high-quality RIRs here: https://www.openair.hosted.york.ac.uk/\n",
    "#    Place a few .wav files from the dataset in the folder specified below.\n",
    "#    If you don't want to use reverb, leave the folder path empty: \"\"\n",
    "RIR_FOLDER = \"rir_filters\"\n",
    "\n",
    "# 3. Set how many augmented versions to create for EACH original audio file.\n",
    "NUM_AUGMENTATIONS_PER_FILE = 3\n",
    "\n",
    "# 4. Set the maximum number of augmentations to chain together in a single pipeline.\n",
    "#    A value of 3 means each new file will have 1, 2, or 3 random effects applied.\n",
    "MAX_AUGMENTATIONS_IN_PIPELINE = 3\n",
    "\n",
    "# --- Define the parameters for each augmentation type ---\n",
    "NOISE_FACTOR_RANGE = (0.005, 0.02)\n",
    "PITCH_STEPS_RANGE = (-4, 4)\n",
    "STRETCH_RATE_RANGE = (0.8, 1.2)\n",
    "SHIFT_MAX_FRACTION = 0.25\n",
    "LOW_PASS_CUTOFF_RANGE = (1000, 4000) # In Hz\n",
    "HIGH_PASS_CUTOFF_RANGE = (200, 750)  # In Hz\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#                         AUGMENTATION FUNCTIONS\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def add_noise(audio, sr, noise_factor):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    return audio + noise_factor * noise\n",
    "\n",
    "def pitch_shift(audio, sr, n_steps):\n",
    "    return librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=n_steps)\n",
    "\n",
    "def time_stretch(audio, sr, rate):\n",
    "    return librosa.effects.time_stretch(y=audio, rate=rate)\n",
    "\n",
    "def time_shift(audio, sr, max_fraction):\n",
    "    shift_range = int(len(audio) * max_fraction)\n",
    "    shift_amount = random.randint(-shift_range, shift_range)\n",
    "    return np.roll(audio, shift_amount)\n",
    "    \n",
    "def apply_reverb(audio, sr, rir_audio):\n",
    "    if len(rir_audio) > len(audio):\n",
    "        rir_audio = rir_audio[:len(audio)] # Truncate RIR if it's longer\n",
    "    rir_normalized = rir_audio / np.max(np.abs(rir_audio))\n",
    "    return convolve(audio, rir_normalized, mode='same')\n",
    "\n",
    "# --- FIX APPLIED HERE ---\n",
    "def dynamic_compress(audio, sr):\n",
    "    \"\"\"\n",
    "    Clips the audio to [-1, 1] before applying mu-law companding. This\n",
    "    prevents errors when prior augmentations push the signal out of range.\n",
    "    The companding is then expanded back to float.\n",
    "    \"\"\"\n",
    "    # Clip audio to the required range for mu-law compression\n",
    "    audio_clipped = np.clip(audio, -1.0, 1.0)\n",
    "    \n",
    "    # Apply mu-law and expand back to float\n",
    "    mu_encoded = librosa.mu_compress(audio_clipped, mu=255)\n",
    "    return librosa.mu_expand(mu_encoded, mu=255)\n",
    "    \n",
    "def apply_filter(audio, sr, cutoff, kind='lowpass', order=5):\n",
    "    nyq = 0.5 * sr\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype=kind, analog=False)\n",
    "    return lfilter(b, a, audio)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#                          MAIN PROCESSING SCRIPT\n",
    "# --------------------------------------------------------------------------\n",
    "# (The rest of the script is unchanged and remains correct)\n",
    "\n",
    "print(\"--- Data Augmentation Script Initializing ---\")\n",
    "\n",
    "# 1. Pre-load RIR files for reverb\n",
    "rir_audios = []\n",
    "if RIR_FOLDER and os.path.exists(RIR_FOLDER):\n",
    "    print(f\"Loading RIR files from: {RIR_FOLDER}\")\n",
    "    for rir_filename in os.listdir(RIR_FOLDER):\n",
    "        if rir_filename.lower().endswith('.wav'):\n",
    "            rir_path = os.path.join(RIR_FOLDER, rir_filename)\n",
    "            try:\n",
    "                rir_audio, rir_sr = librosa.load(rir_path, sr=None, dtype=np.float32)\n",
    "                rir_audios.append(rir_audio)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load RIR file {rir_path}. Error: {e}\")\n",
    "    if not rir_audios:\n",
    "        print(\"Warning: RIR folder specified but no .wav files could be loaded. Reverb will be disabled.\")\n",
    "    else:\n",
    "        print(f\"Successfully loaded {len(rir_audios)} RIR files.\")\n",
    "else:\n",
    "    print(\"Info: RIR folder not found or not specified. Reverb augmentation will be disabled.\")\n",
    "\n",
    "# 2. Collect all original audio files\n",
    "print(\"\\n--- Phase 1: Collecting Original Training Files ---\")\n",
    "original_filepaths = []\n",
    "for root, _, files in os.walk(TRAIN_DATA_FOLDER):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith('.wav') and '_aug_' not in filename:\n",
    "            original_filepaths.append(os.path.join(root, filename))\n",
    "\n",
    "if not original_filepaths:\n",
    "    print(f\"Error: No original .wav files found in '{TRAIN_DATA_FOLDER}'.\")\n",
    "    exit()\n",
    "print(f\"Found {len(original_filepaths)} original audio files to augment.\")\n",
    "\n",
    "# 3. Apply Augmentation Pipelines\n",
    "print(\"\\n--- Phase 2: Applying Augmentation Pipelines ---\")\n",
    "augmentations_applied = 0\n",
    "augmentation_functions = {\n",
    "    'noise': add_noise,\n",
    "    'pitch': pitch_shift,\n",
    "    'stretch': time_stretch,\n",
    "    'shift': lambda audio, sr: time_shift(audio, sr, SHIFT_MAX_FRACTION),\n",
    "    'compress': dynamic_compress,\n",
    "    'lowpass': lambda audio, sr: apply_filter(audio, sr, random.randint(*LOW_PASS_CUTOFF_RANGE), 'lowpass'),\n",
    "    'highpass': lambda audio, sr: apply_filter(audio, sr, random.randint(*HIGH_PASS_CUTOFF_RANGE), 'highpass'),\n",
    "}\n",
    "if rir_audios:\n",
    "    augmentation_functions['reverb'] = lambda audio, sr: apply_reverb(audio, sr, random.choice(rir_audios))\n",
    "function_pool = list(augmentation_functions.items())\n",
    "\n",
    "for filepath in tqdm(original_filepaths, desc=\"Augmenting files\"):\n",
    "    try:\n",
    "        audio, sr = librosa.load(filepath, sr=None, dtype=np.float32)\n",
    "        for i in range(NUM_AUGMENTATIONS_PER_FILE):\n",
    "            augmented_audio = audio.copy()\n",
    "            num_effects_to_apply = random.randint(1, min(MAX_AUGMENTATIONS_IN_PIPELINE, len(function_pool)))\n",
    "            effects_to_apply = random.sample(function_pool, num_effects_to_apply)\n",
    "            applied_names = []\n",
    "            for aug_name, aug_func in effects_to_apply:\n",
    "                applied_names.append(aug_name)\n",
    "                if aug_name == 'noise':\n",
    "                    noise_factor = random.uniform(*NOISE_FACTOR_RANGE)\n",
    "                    augmented_audio = aug_func(augmented_audio, sr, noise_factor)\n",
    "                elif aug_name == 'pitch':\n",
    "                    n_steps = random.choice([s for s in range(PITCH_STEPS_RANGE[0], PITCH_STEPS_RANGE[1] + 1) if s != 0])\n",
    "                    augmented_audio = aug_func(augmented_audio, sr, n_steps)\n",
    "                elif aug_name == 'stretch':\n",
    "                    rate = random.uniform(*STRETCH_RATE_RANGE)\n",
    "                    while np.isclose(rate, 1.0):\n",
    "                        rate = random.uniform(*STRETCH_RATE_RANGE)\n",
    "                    augmented_audio = aug_func(augmented_audio, sr, rate)\n",
    "                else: \n",
    "                    augmented_audio = aug_func(augmented_audio, sr)\n",
    "            base_name = os.path.basename(filepath).split('.wav')[0]\n",
    "            effects_str = '_'.join(sorted(applied_names))\n",
    "            new_filename = f\"{base_name}_aug_{i+1}_{effects_str}.wav\"\n",
    "            output_path = os.path.join(os.path.dirname(filepath), new_filename)\n",
    "            augmented_audio_fp32 = augmented_audio.astype(np.float32)\n",
    "            sf.write(output_path, augmented_audio_fp32, sr)\n",
    "            augmentations_applied += 1\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Could not process file {filepath}. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"       Data augmentation complete! ✨\")\n",
    "print(f\"Successfully created {augmentations_applied} new training files.\")\n",
    "print(f\"Your training folders now contain a diverse mix of augmented data.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de1892bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\sreeh\\appdata\\roaming\\python\\python312\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sreeh\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1015fa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion from .wav audio to 3-channel mel-spectrogram .png images...\n",
      "Input directory: Dataset_For_Training/\n",
      "Output directory: melspectrograms_dataset/\n",
      "Found 4908 audio files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting files: 100%|██████████| 4908/4908 [05:16<00:00, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CONVERSION SUMMARY\n",
      "==================================================\n",
      "Successful conversions: 4908\n",
      "Failed conversions: 0\n",
      "Total files processed: 4908\n",
      "3-channel mel-spectrograms saved in: melspectrograms_dataset/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress common UserWarning from librosa\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. CONFIGURATION\n",
    "# -------------------------------------------------------------------\n",
    "INPUT_AUDIO_DIR = \"Dataset_For_Training/\"\n",
    "OUTPUT_IMAGE_DIR = \"melspectrograms_dataset/\"\n",
    "\n",
    "# Audio Parameters\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = None  # Set to specific seconds if you want fixed-length clips\n",
    "OFFSET = 0.0     # Start reading after this time (seconds)\n",
    "\n",
    "# Spectrogram Parameters\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 256\n",
    "FMAX = 8000\n",
    "\n",
    "# Image settings\n",
    "FIG_SIZE = (5, 5)\n",
    "DPI = 100  # Control image resolution\n",
    "COLORMAP = 'viridis'  # Matplotlib colormap for visualization\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. HELPER FUNCTIONS\n",
    "# -------------------------------------------------------------------\n",
    "def normalize_channel(channel):\n",
    "    \"\"\"Normalizes a single spectrogram channel to the 0-255 range for image saving.\"\"\"\n",
    "    if np.min(channel) == np.max(channel):\n",
    "        return np.zeros_like(channel, dtype=np.uint8)\n",
    "    \n",
    "    scaled = (channel - np.min(channel)) / (np.max(channel) - np.min(channel))\n",
    "    return (scaled * 255).astype(np.uint8)\n",
    "\n",
    "def get_audio_files(directory):\n",
    "    \"\"\"Get all audio files from directory recursively.\"\"\"\n",
    "    audio_extensions = ('.wav', '.flac', '.mp3', '.m4a', '.aac', '.ogg')\n",
    "    audio_files = []\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(audio_extensions):\n",
    "                audio_files.append(os.path.join(dirpath, filename))\n",
    "    return audio_files\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. CONVERSION PROCESS\n",
    "# -------------------------------------------------------------------\n",
    "def main():\n",
    "    print(\"Starting conversion from .wav audio to 3-channel mel-spectrogram .png images...\")\n",
    "    print(f\"Input directory: {INPUT_AUDIO_DIR}\")\n",
    "    print(f\"Output directory: {OUTPUT_IMAGE_DIR}\")\n",
    "    \n",
    "    os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "    \n",
    "    audio_files = get_audio_files(INPUT_AUDIO_DIR)\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(f\"No audio files found in {INPUT_AUDIO_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(audio_files)} audio files to process\")\n",
    "    \n",
    "    successful_conversions = 0\n",
    "    failed_conversions = 0\n",
    "    \n",
    "    for input_filepath in tqdm(audio_files, desc=\"Converting files\"):\n",
    "        try:\n",
    "            # Create corresponding output directory structure\n",
    "            relative_path = os.path.relpath(os.path.dirname(input_filepath), INPUT_AUDIO_DIR)\n",
    "            output_dir_path = os.path.join(OUTPUT_IMAGE_DIR, relative_path)\n",
    "            os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "            base_name, _ = os.path.splitext(os.path.basename(input_filepath))\n",
    "            output_filepath = os.path.join(output_dir_path, f\"{base_name}.png\")\n",
    "\n",
    "            # Load audio with optional duration and offset\n",
    "            y, sr = librosa.load(\n",
    "                input_filepath, \n",
    "                sr=SAMPLE_RATE, \n",
    "                duration=DURATION,\n",
    "                offset=OFFSET\n",
    "            )\n",
    "\n",
    "            # Generate Mel Spectrogram\n",
    "            S = librosa.feature.melspectrogram(\n",
    "                y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, \n",
    "                n_mels=N_MELS, fmax=FMAX\n",
    "            )\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "            # Calculate Deltas (the rate of change)\n",
    "            delta_S_dB = librosa.feature.delta(S_dB)\n",
    "            delta2_S_dB = librosa.feature.delta(S_dB, order=2)\n",
    "            \n",
    "            # Normalize each channel to the 0-255 range\n",
    "            S_dB_norm = normalize_channel(S_dB)\n",
    "            delta_S_dB_norm = normalize_channel(delta_S_dB)\n",
    "            delta2_S_dB_norm = normalize_channel(delta2_S_dB)\n",
    "            \n",
    "            # Stack them into a 3-channel array (height, width, 3)\n",
    "            stacked_image_data = np.stack([S_dB_norm, delta_S_dB_norm, delta2_S_dB_norm], axis=-1)\n",
    "\n",
    "            # Save the 3-channel spectrogram as a borderless image\n",
    "            plt.figure(figsize=FIG_SIZE, dpi=DPI)\n",
    "            plt.imshow(stacked_image_data, aspect='auto', cmap=COLORMAP)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout(pad=0)\n",
    "            plt.savefig(output_filepath, bbox_inches='tight', pad_inches=0, dpi=DPI)\n",
    "            plt.close()\n",
    "            \n",
    "            successful_conversions += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n-!> ERROR processing {input_filepath}: {e}\")\n",
    "            failed_conversions += 1\n",
    "            continue\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CONVERSION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Successful conversions: {successful_conversions}\")\n",
    "    print(f\"Failed conversions: {failed_conversions}\")\n",
    "    print(f\"Total files processed: {len(audio_files)}\")\n",
    "    print(f\"3-channel mel-spectrograms saved in: {OUTPUT_IMAGE_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
